{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Openwebtext\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 4\n",
    "block_size = 1024\n",
    "vocabulary_size = Openwebtext.VOCABULARY_SIZE\n",
    "embedding_dimensions = 768\n",
    "num_hidden_layers = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll estimate the total number of parameters in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings               38,598,144      31.04%\n",
      "position_embeddings               786,432       0.63%\n",
      "multihead_attention            28,311,552      22.77%\n",
      "mlp                            56,623,104      45.54%\n",
      "layer_norm                         19,200       0.02%\n",
      "\n",
      "\n",
      "Total parameters: 124,338,432\n"
     ]
    }
   ],
   "source": [
    "parameter_counts = {\n",
    "    \"token_embeddings\": vocabulary_size * embedding_dimensions,\n",
    "    \"position_embeddings\": block_size * embedding_dimensions,\n",
    "    \"multihead_attention\": (embedding_dimensions ** 2 + embedding_dimensions * 3 * embedding_dimensions) * num_hidden_layers,\n",
    "    \"mlp\": embedding_dimensions * 4 * embedding_dimensions * 2 * num_hidden_layers,\n",
    "    \"layer_norm\": embedding_dimensions * num_hidden_layers * 2 + embedding_dimensions,\n",
    "}\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the size of the model in memory and on disk. Note that this does not include any intermediate variables that get memorized during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gigabytes: 1.49\n"
     ]
    }
   ],
   "source": [
    "bytes_per_parameter = 32 // 8 # Assuming 32-bit floating point\n",
    "\n",
    "buffers_per_parameter = 2 # Assuming AdamW optimizer\n",
    "\n",
    "total_values = total_parameter_count + buffers_per_parameter * total_parameter_count\n",
    "\n",
    "total_bytes_per_parameter = total_values * bytes_per_parameter\n",
    "\n",
    "total_gigabytes = total_bytes_per_parameter / 1e9\n",
    "\n",
    "print(f\"Total gigabytes: {total_gigabytes:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the number of floating point operations (FLOPs) required to perform a full forward pass of the network. Note that we do not include layer norm operations in this estimate as they are negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention                  96,636,764,160      33.13%\n",
      "mlp                       115,964,448,768      39.76%\n",
      "output_layer               79,048,998,912      27.10%\n",
      "\n",
      "\n",
      "Total forward FLOPs: 291,650,211,840\n"
     ]
    }
   ],
   "source": [
    "ops_per_matmul = 2 # Multiply + accumulate\n",
    "ops_per_activation = 9 # Assuming GELU\n",
    "\n",
    "# K, Q, V projections\n",
    "attention = ops_per_matmul * block_size * embedding_dimensions * 3 * embedding_dimensions\n",
    "\n",
    "# Attention logits\n",
    "attention += 2 * ops_per_matmul * block_size ** 2 * embedding_dimensions\n",
    "\n",
    "# Output projection\n",
    "attention += ops_per_matmul * block_size * embedding_dimensions ** 2\n",
    "\n",
    "attention *= num_hidden_layers\n",
    "\n",
    "# Linear transformations\n",
    "mlp = 2 * ops_per_matmul * block_size * embedding_dimensions * 4 * embedding_dimensions\n",
    "\n",
    "# Non-linear activations\n",
    "mlp += ops_per_activation * 4 * embedding_dimensions\n",
    "\n",
    "mlp *= num_hidden_layers\n",
    "\n",
    "output_layer = ops_per_matmul * block_size * embedding_dimensions * vocabulary_size\n",
    "\n",
    "flops = {\n",
    "    \"attention\": attention,\n",
    "    \"mlp\": mlp,\n",
    "    \"output_layer\": output_layer,\n",
    "}\n",
    "\n",
    "total_forward_flops = sum(flops.values())\n",
    "\n",
    "for name, count in flops.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_forward_flops * 100:10.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total forward FLOPs: {total_forward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the number of FLOPs for the backward pass. For this we use a simple heuristic of 2X the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total backward FLOPs: 583,300,423,680\n"
     ]
    }
   ],
   "source": [
    "total_backward_flops = 2 * total_forward_flops\n",
    "\n",
    "print(f\"Total backward FLOPs: {total_backward_flops:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
